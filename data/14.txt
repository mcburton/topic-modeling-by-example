One of the most prevalent debates within the Digital Humanities (DH) is the idea that practitioners should just go about doing rather than talking, or to practice “more hack, less yack.” In other words, instead of pontificating and problematizing, DH scholars should be more concerned with making stuff, and making stuff happen. The “more hack, less yack” mantra has been going on for a while now, and has brushed up against some challenges; notably Natalia Cecire’s (@ncecire) argument for the need for a ThatCamp Theory to uncover the theoretical leanings of the digital humanities, in Alan Liu’s call for the need to integrate cultural studies into dh approaches, and in the recent TransformDH collective, set up by Anne Cong-Huyen (@anitaconchita), Moya Bailey (@moyazb) and M. Rivera Monclova (@phdeviate) to bring race/gender/class/disability criticism to the digital humanities.** In many of these debates, it seems as though the “theoretification” of DH is viewed with suspicion as it disturbs the implicit good nature of much of the DH community. Roger Whitson (@rogerwhitson), for example, mused on whether the digital humanities “really needs to be transformed,” arguing that: “It seems to me that the word “guerilla” reappropriates the collaborative good will of the digital humanities, making it safe for traditional academic consumption and inserting it into the scheme Stanley Fish and William Pannapacker (@pannapacker) highlight.”
I’ve been musing on the “more hack, less yack” issue recently, and it seems that Tara McPherson’s (@tmcphers) essay “U.S. Operating Systems at Mid-Century: The Intertwining of Race and UNIX” in Lisa Nakamura (@lnakamur) and Peter Chow-White’s (@pachowwhite) recent collectionRace After the Internet may offer some important insights into this ideological impasse. In her essay, McPherson argues that in the mid-twentieth century, a common structural logic developed due to computerization, one which argued for the importance of “modular thinking”, “common-sense” and disciplinary hyperspecialization. By focusing on processes which work via the modular form—simple blocks by which a complex system is broken down and analyzed by individual groups—the rationale of this system appears “common-sensical”, thereby obscuring the actual political and social moment from which it emerges.
McPherson sees this modular logic manifest in both the development of UNIX as well as racial formations in the United States, and expands this to argue that this might be a hallmark of the Fordist moment of capitalist production in the United States, and finds its manifestation in the hyperspecialization of late capitalism, extending to the specialization of disciplines in higher education such as Area Studies departments. This mode of modular thinking, she argues, is a type of “lenticular logic” which undergirds both the structures of UNIX as well as the covert racism of color blindness:
“A lenticular logic is a covert racial logic, a logic for the post-Civil Rights era. We might contrast the lenticular postcard to that wildly popular artifact of the industrial era, the stereoscope card. The stereoscope melds two different images into an imagined whole, privileging the whole; the lenticular image partitions and divides, privileging fragmentation. A lenticular logic is a logic of the fragment or the chunk, a way of seeing the world as discrete modules or nodes, a mode that suppresses relation and context. As such, the lenticular also manages and controls complexity.” (25)
Reading McPherson makes me think: to what degree does lenticular logic underlie the DH imperative for “more hack, less yack?” How much does digital humanities work, through the way it is processed and organized through computational models, actually follow the Fordist logic of modularity? In the same way that UNIX engineers extolled programmers to “common sense and notions about simplicity to justify their innovations in code,” (28), neglecting how his common-sense is similarly constituted by their historical specificity as a class of workers in the 1970s, how has this sentiment actually provided the language behind “more hack, less yack?”
In other words, common sense is never simply “common sense.” What is “common sense” comes out of a particular socio-historical moment, just as “hacking” has derived from a very specific social context. And, just as UNIX programmers relied, in McPherson’s argument, on a common-sense modular “lenticular logic” to avoid speaking about the socio-political origins and conditions that allowed for their “common sense” to come into being, perhaps the same logic has underwritten our resistance to theory within the digital humanities. Where does our “common sense” in the digital humanities come from? How is it implicated in structures of privilege which remain invisible to us? Why are we so resistant to speaking about it, and how does the language of modularity aid us in this silence?
It appears to me that much of the “more hack, less yack” issue circles around the problem of modularity and common-sensical “form” that McPherson outlines in this essay. I see this in Bethany Nowviskie’s (@nowviskie) recent post, Don’t Circle the Wagons:
“Software development functions in relative silence within the larger conversation of the digital humanities, in a sub-culture suffused — in my experience — not with locker-room towel-snaps and construction-worker catcalls, but with lovely stuff that’s un-voiced: what Bill Turkel and Devon Elliott have calledtacit understanding, and with journeyman learning experiences. And that’s no surprise. To my mind, coding itself has more in common with traditional arts-and-crafts practice than with academic discourse.Too often, the things developers know — know and value, because they enact them every day — go entirely unspoken. These include the philosophy and ethos of software craftsmanship and, by extension, the intellectual underpinnings of digital humanities practice. (If you use their tools, that goes for your DH practice, too.)”
Nowviskie’s elaboration of a “tacit understanding” that derives from “journeyman learning experiences” makes me wonder how much of these learning experiences dovetail with McPherson’s notion of modular, lenticular logic that structures UNIX and other mid-century structuralist Fordist systems. This “tacit understanding” creates a common-sense notion of simplicity, but one whose structure and “common-sensical” nature similarly allows for a significant amnesia towards its own socio-political origins and context. In the same way that UNIX engineers extolled programmers to “common sense and notions about simplicity to justify their innovations in code,” (McPherson 28), neglecting how his common-sense is similarly constituted by their historical specificity as a class of workers in the 1970s, how has this mode of thought provided the language for the “more hack, less yack” sentiment?
McPherson’s argument recalls Paul De Man’s Blindness and Insight, where De Man asserted that all critical readings are ultimately predicated upon a “negative movement that animates the critic’s thought, an unstated principle that leads his language away from its asserted stand… as if the very possibility of assertion had been put into question.” De Man argued that we needed to return to engaging how a certain type of form made certain readings possible. At the same time, he asserted that the blindness to that very form was critical to structuring our insights. While De Man’s metaphor is problematically ableist***, it still makes a critical point: that we need to interrogate how the logic of form tends to erase the perspective of its own creation. As literary theory given critics insights that hide their own foundations, the logics of computation have given us a certain type of structure, a type of tacit understanding, a sort of visible logic and knowing that have simultaneously obscured their own foundational assumptions.
I do not mean to suggest that tacit understanding equates to a certain type of blindness. That said, I do mean to recognize that all forms of shared, cultural understandings, whether they come under the umbrella terms “common sense,” “tradition” or “ritual,” are founded upon an important obscuring of their own particular socio-political specificity, and that to ignore this specificity is troubling. As Pierre Bourdieu observed, all cultural practices exist as habitus, a set of learned dispositions, skills and ways of acting that appear simply natural, but which are rooted in specific social-cultural contexts. My call, then, is for us to interrogate the habitus that makes up the Digital Humanities community.
Let me be clear. I get annoyed by jargon and obfuscation as much as the next person, which is why I am so attracted to the digital humanities community. But I do think that we need to invest in the creation of a metalanguage that will allow us to see the ideological foundations that undergird our “common sense.” And sometimes that comes hand in hand with theory. Also, theory doesn’t always need to be annoyingly grating, especially if it allows us to understand how our implicit systems invisibly privilege and disenfranchise certain groups of people. We need to question the forms that make us see “common-sense”, and to see value in the converse “less hack, more yack” proposition.
If computation is, as Cathy N. Davidson (@cathyndavidson) and Dan Rowinski have been arguing, the fourth “R” of 21st century literacy, we very much need to treat it the way we already do existing human languages: as modes of knowledge which unwittingly create cultural valences and systems of privilege and oppression. Frantz Fanon wrote inBlack Skin, White Masks: “To speak a language is to take on a world, a civilization.” As Digital Humanists, we have the responsibility to interrogate and to understand what kind of world, and what kind of civilization, our computational languages and forms create for us. Critical Code Studies is an important step in this direction. But it’s important not to stop there, but to continue to try to expand upon how computation and the digital humanities are underwritten by theoretical suppositions which still remain invisible.
** Alexis Lothian’s article, “Marked Bodies, Transformative Scholarship and the Question of Theory in the Digital Humanities.” Journal of Digital Humanities 1:1, November 4, 2011, gives an excellent history of the #TransformDH group, and the call towards Theory within the Digital Humanities. Thanks Alexis (@alothian) for pointing me to this!
***Thanks to Natalia Cecire (@ncecire) for reminding me of this.
Image Credit
Edited to Add: Some interesting responses to this post